{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm comparison \n",
    "This notebook contains the algorithm comparison for the measurements done with the DySpan 2017 testbed\n",
    "\n",
    "N_FRAMES = 50\n",
    "comparison is based on different aspects, such as:\n",
    "\n",
    "* Dataset lenght\n",
    "* complexity: different for each algorithm, made by playing around with the parameters that it has available\n",
    "* Pre- and post- feature scaling\n",
    "\n",
    "Aspects that are better explained in the thesis document contained in this same repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.13 (default, May 10 2017, 20:04:28) \n",
      "[GCC 6.3.1 20161221 (Red Hat 6.3.1-1)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../python\")\n",
    "import setup_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "106890\n",
      "106890\n"
     ]
    }
   ],
   "source": [
    "data, labels = setup_dataset.setup_simple_iterables(\"with_dc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = setup_dataset.slice_data(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up various complexities for the different algorithms.\n",
    "# Number of neighbors\n",
    "knn_c = (2, 4, 10, 50)\n",
    "# Maximum depth in a decision tree\n",
    "dtc_c = (2, 5, 10, 50)\n",
    "# complexities for the rbf kernel\n",
    "svc_c = (1, 1000, 1000000)\n",
    "# Number of estimators in the random forest classifier\n",
    "rfc_c = (1, 10, 100, 1000, 10000, 100000)\n",
    "# Number of parallel jobs (CPU)\n",
    "rfc_jobs = (3, -2)\n",
    "gpc_jobs = (3, -2)\n",
    "# Number of iteration in the Gaussian Process Classifier\n",
    "gpc_c = (20, 50, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler, X_train_scaled, X_test_scaled = setup_dataset.scale_sliced_data(X_train, X_test, StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_list, knn_accs, knn_pred, knn_pred_times, knn_fit_times = \\\n",
    "setup_dataset.run_knn(X_train, X_test, y_train, y_test, knn_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "setup_dataset.compute_cm(y_test, knn_pred, knn_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_list_scaled, knn_accs_scaled, knn_pred_scaled, knn_pred_times_scaled, knn_fit_times_scaled =\\\n",
    "setup_dataset.run_knn(X_train_scaled, X_test_scaled, y_train, y_test, knn_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setup_dataset.compute_cm(y_test, knn_pred_scaled, knn_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in knn_accs :\n",
    "    print(line)\n",
    "print(\"====================\") \n",
    "for line in knn_accs_scaled:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtc_list, dtc_accs, dtc_pred, dtc_pred_times, dtc_fit_times = \\\n",
    "setup_dataset.run_decision_tree(X_train, X_test, y_train, y_test, dtc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtc_list_scaled, dtc_accs_scaled, dtc_pred_scaled, dtc_pred_times_scaled, dtc_fit_times_scaled = \\\n",
    "setup_dataset.run_decision_tree(X_train_scaled, X_test_scaled, y_train, y_test, dtc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "setup_dataset.compute_cm(y_test, dtc_pred, dtc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "setup_dataset.compute_cm(y_test, dtc_pred_scaled, dtc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in dtc_accs :\n",
    "    print(line)\n",
    "print(\"====================\") \n",
    "for line in dtc_accs_scaled:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbc_list, nbc_accs, nbc_pred, nbc_pred_times, nbc_fit_times = \\\n",
    "setup_dataset.run_naive_bayes(X_train, X_test, y_train, y_test, (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbc_list_scaled, nbc_accs_scaled, nbc_pred_scaled, nbc_pred_times_scaled, nbc_fit_times_scaled = \\\n",
    "setup_dataset.run_naive_bayes(X_train_scaled, X_test_scaled, y_train, y_test, (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "setup_dataset.compute_cm(y_test, nbc_pred, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setup_dataset.compute_cm(y_test, nbc_pred_scaled, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abc_list, abc_accs, abc_pred, abc_pred_times, abc_fit_times = \\\n",
    "setup_dataset.run_adaboost(X_train, X_test, y_train, y_test, (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abc_list_scaled, abc_accs_scaled, abc_pred_scaled, abc_pred_times_scaled, abc_fit_times_scaled = \\\n",
    "setup_dataset.run_adaboost(X_train_scaled, X_test_scaled, y_train, y_test, (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "setup_dataset.compute_cm(y_test, abc_pred, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setup_dataset.compute_cm(y_test, abc_pred_scaled, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qda_list, qda_accs, qda_pred, qda_pred_times, qda_fit_times = \\\n",
    "setup_dataset.run_quadratic(X_train, X_test, y_train, y_test, (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qda_list_scaled, qda_accs_scaled, qda_pred_scaled, qda_pred_times_scaled, qda_fit_times_scaled = \\\n",
    "setup_dataset.run_quadratic(X_train_scaled, X_test_scaled, y_train, y_test, (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setup_dataset.compute_cm(y_test, qda_pred, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setup_dataset.compute_cm(y_test, qda_pred_scaled, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_list, svc_accs, svc_pred, svc_pred_times, svc_fit_times = \\\n",
    "setup_dataset.run_svc(X_train, X_test, y_train, y_test, svc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_list_scaled, svc_accs_scaled, svc_pred_scaled, svc_pred_times_scaled, svc_fit_times_scaled = \\\n",
    "setup_dataset.run_svc(X_train_scaled, X_test_scaled, y_train, y_test, svc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setup_dataset.compute_cm(y_test, svc_pred, svc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setup_dataset.compute_cm(y_test, svc_pred_scaled, svc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in svc_accs :\n",
    "    print(line)\n",
    "print(\"====================\") \n",
    "for line in svc_accs_scaled:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in svc_accs :\n",
    "    print(line)\n",
    "print(\"====================\") \n",
    "for line in svc_accs_scaled:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THIS MAKES THE KERNEL CRASH!\n",
    "rfc_list, rfc_accs, rfc_pred, rfc_pred_times, rfc_fit_times = \\\n",
    "setup_dataset.run_random_forest(X_train, X_test, y_train, y_test, rfc_c, rfc_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc_list_scaled, rfc_accs_scaled, rfc_pred_scaled, rfc_pred_times_scaled, rfc_fit_times_scaled = \\\n",
    "setup_dataset.run_random_forest(X_train_scaled, X_test_scaled, y_train, y_test, rfc_c, rfc_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setup_dataset.compute_cm(y_test, rfc_pred, rfc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setup_dataset.compute_cm(y_test, rfc_pred_scaled, rfc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpc_list, gpc_accs, gpc_pred, gpc_pred_times, gpc_fit_times = \\\n",
    "setup_dataset.run_gaussian(X_train, X_test, y_train, y_test, gpc_c, gpc_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpc_list_scaled, gpc_accs_scaled, gpc_pred_scaled, gpc_pred_times_scaled, gpc_fit_times_scaled = \\\n",
    "setup_dataset.run_gaussian(X_train_scaled, X_test_scaled, y_train, y_test, gpc_c, rfc_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setup_dataset.compute_cm(y_test, gpc_pred, gpc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setup_dataset.compute_cm(y_test, gpc_pred_scaled, gpc_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder for small graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "x = np.arange(len(knn_accs[0]))\n",
    "y = [[] for _ in range(len(knn_accs[0]))]\n",
    "for i in range(len(knn_accs[0])):\n",
    "    y[i] = knn_accs[i]\n",
    "    plt.plot(x, y[i], linestyle='-', label=\"complexity {}\".format(i))\n",
    "    # plt.scatter(x, y[i], label=\"data {}\".format(i))\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x = np.arange(len(knn_accs[0]))\n",
    "y = [[] for _ in range(len(knn_accs[0]))]\n",
    "width = 0.2\n",
    "for i in range(len(knn_accs[0])):\n",
    "    y[i] = knn_accs[i]\n",
    "    plt.bar(x- 1.5*width + width*i, y[i], width, align='center', label=\"complexity {}\".format(i), alpha=0.8)\n",
    "    # plt.scatter(x, y[i], label=\"data {}\".format(i))\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x = np.arange(len(knn_fit_times[0]))\n",
    "y = [[] for _ in range(len(knn_fit_times[0]))]\n",
    "for i in range(len(knn_fit_times[0])):\n",
    "    y[i] = knn_fit_times[i]\n",
    "    plt.plot(x, y[i], linestyle='-', label=\"complexity {}\".format(i))\n",
    "    # plt.scatter(x, y[i], label=\"data {}\".format(i))\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x = np.arange(len(knn_accs_scaled[0]))\n",
    "y = [[] for _ in range(len(knn_accs_scaled[0]))]\n",
    "for i in range(len(knn_accs_scaled[0])):\n",
    "    y[i] = knn_accs_scaled[i]\n",
    "    plt.plot(x, y[i], linestyle='-', label=\"complexity {}\".format(i))\n",
    "    # plt.scatter(x, y[i], label=\"data {}\".format(i))\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x = np.arange(len(svc_accs[0]))\n",
    "y = [[] for _ in range(len(svc_accs[0]))]\n",
    "for i in range(len(svc_accs[0])):\n",
    "    y[i] = svc_accs[i]\n",
    "    plt.plot(x, y[i], linestyle='-', label=\"complexity {}\".format(i))\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x = np.arange(len(svc_accs_scaled[0]))\n",
    "y = [[] for _ in range(len(svc_accs_scaled[0]))]\n",
    "for i in range(len(svc_accs_scaled[0])):\n",
    "    y[i] = svc_accs_scaled[i]\n",
    "    plt.plot(x, y[i], linestyle='-', label=\"complexity {}\".format(i))\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x = np.arange(len(dtc_accs_scaled[0]))\n",
    "y = [[] for _ in range(len(dtc_accs_scaled[0]))]\n",
    "for i in range(len(dtc_accs_scaled[0])):\n",
    "    y[i] = dtc_accs_scaled[i]\n",
    "    plt.plot(x, y[i], linestyle='-', label=\"complexity {}\".format(i))\n",
    "    # plt.scatter(x, y[i], label=\"data {}\".format(i))\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in dtc_accs :\n",
    "    print(line)\n",
    "print(\"====================\") \n",
    "for line in dtc_accs_scaled:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the models to disk\n",
    "\n",
    "We choose the models with a  good performance, and we save the models to disk in order to use them in a live implementation. For this, we are going to use the \"pickle\" library that allows us to dump the model into a file. \n",
    "\n",
    "Based on the results recorded in this notebook, we are going to save the following models:\n",
    "* k-nearest neighbors, not staled, with the whole data set and 4 neighbors\n",
    "* decision tree, not staled, with whole data set and depth=50\n",
    "* Support vector classifier, scaled, with whole data and a RBF kernel complexity of 1e6\n",
    "\n",
    "**NOTE**: Be sure of pickle (save) the models in the same python version that you are going to use for unplickle it later on. As I will be using GNURadio (at its master branch), it is a requirement that this notebook runs with a Python2 kernel. For issues changing the kernel please [see this thread](https://stackoverflow.com/questions/30492623/using-both-python-2-x-and-python-3-x-in-ipython-notebook/37857536#37857536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d39f4ab2c2d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../weights/knn_full_data_set_4_neighbors.sav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtc_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../weights/dtc_full_data_set_depth_50.sav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvc_list_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../weights/svc_full_data_set_rbf_1e6.sav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "pickle.dump(knn_list[3][1], open('../weights/knn_full_data_set_4_neighbors.sav', 'wb'))\n",
    "pickle.dump(dtc_list[3][3], open('../weights/dtc_full_data_set_depth_50.sav', 'wb'))\n",
    "pickle.dump(svc_list_scaled[3][2], open('../weights/svc_full_data_set_rbf_1e6.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be done without additional dependencies using _joblib_, which is shipped with scikit-learn. This has the advantage of being able to take either a file object or just the path to the file as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../weights/svc_full_data_set_rbf_1e6.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn_list[3][1], '../weights/knn_full_data_set_4_neighbors.pkl', protocol=2)\n",
    "joblib.dump(dtc_list[3][3], '../weights/dtc_full_data_set_depth_50.pkl', protocol=2)\n",
    "joblib.dump(svc_list_scaled[3][2], '../weights/svc_full_data_set_rbf_1e6.pkl', protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joblib works wonders with sklearn, as well as with numpy arrays in general. For general purposes they both do their job just fine (as joblib uses pickle in the background), but [joblib can be significantly faster](https://gist.github.com/vsoch/61a82e7c920468325ea8)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the validity of the saved models\n",
    "\n",
    "Now we just want to double-check that the models that we just saved are according to the values that we saw just after the learning process, and before savind the model persistance. In order to do this, we just load the model into a new variable from the saved file, and check for the accuracy of the new model using the same test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, remember that our X_train, X_test, y_train and y_test has been sliced in order to simulate data sets of variable lenghts. So, we take the largest as it demostrated to provide better results in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_large = X_test[3]\n",
    "X_test_scaled_large = X_test_scaled[3]\n",
    "y_test_large = y_test[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_saved = pickle.load(open('../weights/knn_full_data_set_4_neighbors.sav', 'rb'))\n",
    "dtc_saved = pickle.load(open('../weights/dtc_full_data_set_depth_50.sav', 'rb'))\n",
    "svc_saved = pickle.load(open('../weights/svc_full_data_set_rbf_1e6.sav', 'rb'))\n",
    "\n",
    "print(\"The score achieved with the saved model is:\\n\")\n",
    "print(\"K-nearest Neighbors =\", knn_saved.score(X_test_large, y_test_large))\n",
    "print(\"Decision Tree =\", dtc_saved.score(X_test_large,y_test_large))\n",
    "print(\"Support Vector Machine =\", svc_saved.score(X_test_scaled_large,y_test_large))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_saved = joblib.load('../weights/knn_full_data_set_4_neighbors.pkl')\n",
    "dtc_saved = joblib.load('../weights/dtc_full_data_set_depth_50.pkl')\n",
    "svc_saved = joblib.load('../weights/svc_full_data_set_rbf_1e6.pkl')\n",
    "\n",
    "result = knn_saved.score(X_test_large,y_test_large)\n",
    "print(\"The score achieved with the saved model is:\\n\")\n",
    "print(\"K-nearest Neighbors =\", knn_saved.score(X_test_large, y_test_large))\n",
    "print(\"Decision Tree =\", dtc_saved.score(X_test_large,y_test_large))\n",
    "print(\"Support Vector Machine =\", svc_saved.score(X_test_scaled_large,y_test_large))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.722324, -1.0, 4.5929675, -1.0, 258.44913, 107.17783]\n",
      "26723\n"
     ]
    }
   ],
   "source": [
    "print X_test_large[4000]\n",
    "print len(X_test_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_saved = joblib.dump(scaler, '../weights/scaler_saved.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
